{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Deliverable URL`:\n",
        "https://colab.research.google.com/drive/1b9Sitk8VgLx4KkiFB68wkUVSvNWAJcS7?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eEbVsy0H99u",
        "outputId": "0c0b219d-44e2-4bbe-fe24-c8758ffc546c"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1465806638.py, line 1)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mwget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/refs/heads/main/Viterbi_assignment/train_data.txt\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/refs/heads/main/Viterbi_assignment/train_data.txt\n",
        "!wget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/refs/heads/main/Viterbi_assignment/test_data.txt\n",
        "!wget https://raw.githubusercontent.com/debajyotimaz/nlp_assignment/refs/heads/main/Viterbi_assignment/noisy_test_data.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-7XRbJKM2-j",
        "outputId": "a925fb3e-b540-456d-acb8-707c2e91afc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('He', 'PRON'), ('let', 'VERB'), ('her', 'PRON'), ('tell', 'VERB'), ('him', 'PRON'), ('all', 'PRT'), ('about', 'ADP'), ('the', 'DET'), ('church', 'NOUN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "def load_data(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            sentence = []\n",
        "            for token in line.strip().split():\n",
        "                word, tag = token.rsplit('/', 1)  # Split word and tag\n",
        "                sentence.append((word, tag))\n",
        "            data.append(sentence)\n",
        "    return data\n",
        "\n",
        "# Load train and test data from files\n",
        "train_data_file = '/content/train_data.txt'  # Path to your training data file\n",
        "test_data_file = '/content/test_data.txt'    # Path to your test data file\n",
        "noisy_test_data_file = '/content/noisy_test_data.txt'  # Path to your noisy test data file\n",
        "\n",
        "train_data = load_data(train_data_file)\n",
        "test_data = load_data(test_data_file)\n",
        "noisy_test_data = load_data(noisy_test_data_file)\n",
        "\n",
        "# Print a sample from the training data\n",
        "print(train_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6fBG2exLhAh8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class HMMViterbiPOS:\n",
        "    def __init__(self, train_file, test_file, noisy_test_file):\n",
        "        self.training_sentences = self.load_data(train_file)\n",
        "        self.test_sentences = self.load_data(test_file)\n",
        "        self.noisy_test_sentences = self.load_data(noisy_test_file)\n",
        "        self.states = set()\n",
        "        self.word_set = set()\n",
        "        self.transition_probs = defaultdict(lambda: defaultdict(float))\n",
        "        self.emission_probs = defaultdict(lambda: defaultdict(float))\n",
        "        self.start_probs = defaultdict(float)\n",
        "        self.train_hmm()\n",
        "\n",
        "    def load_data(self, file_path):\n",
        "        data = []\n",
        "        try:\n",
        "            with open(file_path, 'r') as file:\n",
        "                for line in file:\n",
        "                    sentence = [(token.rsplit('/', 1)[0], token.rsplit('/', 1)[1])\n",
        "                                for token in line.strip().split()]\n",
        "                    data.append(sentence)\n",
        "        except FileNotFoundError:\n",
        "            raise Exception(f\"File {file_path} not found\")\n",
        "        return data\n",
        "\n",
        "    def train_hmm(self):\n",
        "        transition_counts = defaultdict(lambda: defaultdict(int))\n",
        "        emission_counts = defaultdict(lambda: defaultdict(int))\n",
        "        start_counts = defaultdict(int)\n",
        "        state_counts = defaultdict(int)\n",
        "\n",
        "        for sentence in self.training_sentences:\n",
        "            previous_tag = None\n",
        "            for word, tag in sentence:\n",
        "                self.states.add(tag)\n",
        "                self.word_set.add(word)\n",
        "                state_counts[tag] += 1\n",
        "                emission_counts[tag][word] += 1\n",
        "\n",
        "                if previous_tag is None:\n",
        "                    start_counts[tag] += 1\n",
        "                else:\n",
        "                    transition_counts[previous_tag][tag] += 1\n",
        "                previous_tag = tag\n",
        "\n",
        "        total_sentences = len(self.training_sentences)\n",
        "        self.start_probs = {tag: (start_counts[tag] + 1) / (total_sentences + len(self.states)) for tag in self.states}\n",
        "\n",
        "        for prev_tag in self.states:\n",
        "            total_transitions = sum(transition_counts[prev_tag].values()) + len(self.states)\n",
        "            for tag in self.states:\n",
        "                self.transition_probs[prev_tag][tag] = (transition_counts[prev_tag][tag] + 1) / total_transitions\n",
        "\n",
        "        for tag in self.states:\n",
        "            total_emissions = sum(emission_counts[tag].values()) + len(self.word_set)\n",
        "            for word in self.word_set:\n",
        "                self.emission_probs[tag][word] = (emission_counts[tag][word] + 1) / total_emissions\n",
        "\n",
        "    def viterbi_algorithm(self, sentence):\n",
        "        viterbi_matrix = [{}]\n",
        "        backpointer = [{}]\n",
        "\n",
        "        for state in self.states:\n",
        "            viterbi_matrix[0][state] = np.log(self.start_probs[state]) + np.log(self.emission_probs[state].get(sentence[0], 1e-6))\n",
        "            backpointer[0][state] = None\n",
        "\n",
        "        for t in range(1, len(sentence)):\n",
        "            viterbi_matrix.append({})\n",
        "            backpointer.append({})\n",
        "\n",
        "            for state in self.states:\n",
        "                max_prob, best_prev_state = max(\n",
        "                    (viterbi_matrix[t - 1][prev_state] + np.log(self.transition_probs[prev_state].get(state, 1e-6)) +\n",
        "                     np.log(self.emission_probs[state].get(sentence[t], 1e-6)), prev_state)\n",
        "                    for prev_state in self.states\n",
        "                )\n",
        "\n",
        "                viterbi_matrix[t][state] = max_prob\n",
        "                backpointer[t][state] = best_prev_state\n",
        "\n",
        "        best_final_state = max(self.states, key=lambda state: viterbi_matrix[-1][state])\n",
        "        best_path = []\n",
        "\n",
        "        for t in reversed(range(len(sentence))):\n",
        "            best_path.insert(0, best_final_state)\n",
        "            best_final_state = backpointer[t][best_final_state]\n",
        "\n",
        "        return list(zip(sentence, best_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23eBoIIph4lS"
      },
      "source": [
        "###Evaluation\n",
        "An example of evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWy2eyhCfr4S",
        "outputId": "7fceb342-8ba2-4471-e3a6-8f63a6bc368a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Viterbi Accuracy: 76.92%\n"
          ]
        }
      ],
      "source": [
        "sentence = [('Get', 'VERB'), ('copper', 'NOUN'), ('or', 'CONJ'), ('earthenware', 'NOUN'), ('mugs', 'NOUN'), ('that', 'PRON'), ('keep', 'VERB'), ('beer', 'NOUN'), ('chilled', 'VERB'), ('or', 'CONJ'), ('soup', 'NOUN'), ('hot', 'ADJ'), ('.', '.')]\n",
        "predicted_tags = ['DET', 'NOUN', 'CONJ', 'NOUN', 'ADP', 'PRON', 'VERB', 'NOUN', '.', 'CONJ', 'NOUN', 'ADJ', '.']\n",
        "true_tags = ('VERB', 'NOUN', 'CONJ', 'NOUN', 'NOUN', 'PRON', 'VERB', 'NOUN', 'VERB', 'CONJ', 'NOUN', 'ADJ', '.')\n",
        "correct = 0\n",
        "total = 0\n",
        "correct += sum(p == t for p, t in zip(predicted_tags, true_tags))\n",
        "total += len(true_tags)\n",
        "accuracy = correct / total\n",
        "\n",
        "print(f\"Baseline Viterbi Accuracy: {accuracy * 100:.2f}%\")\n",
        "# print(f\"Viterbi with Noise Handling Accuracy: {accuracy * 100:.2f}%\")  # similarly calculate for Noise Handling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nH7DessOTSd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAzM8GusGBzF",
        "outputId": "a0b70460-9cca-4123-80a6-8858e5dbb45a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Viterbi Algorithm Accuracy: 83.45%\n"
          ]
        }
      ],
      "source": [
        "# Function to evaluate accuracy\n",
        "def evaluate_accuracy(hmm_model, test_data):\n",
        "    correct, total = 0, 0\n",
        "    predictions = []\n",
        "\n",
        "    for sentence in test_data:\n",
        "        words = [word for word, _ in sentence]\n",
        "        predicted_tags = hmm_model.viterbi_algorithm(words)\n",
        "        predictions.append(predicted_tags)\n",
        "\n",
        "        for (pred_word, pred_tag), (true_word, true_tag) in zip(predicted_tags, sentence):\n",
        "            if pred_tag == true_tag:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "# Load the trained HMM model\n",
        "hmm_model = HMMViterbiPOS(\"/content/train_data.txt\", \"/content/test_data.txt\", \"/content/noisy_test_data.txt\")\n",
        "\n",
        "# Evaluate accuracy on test and noisy test data\n",
        "test_accuracy = evaluate_accuracy(hmm_model, hmm_model.test_sentences)\n",
        "noisy_test_accuracy = evaluate_accuracy(hmm_model, hmm_model.noisy_test_sentences)\n",
        "\n",
        "test_accuracy, noisy_test_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HbIp-vpGGEYN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "MYENV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
